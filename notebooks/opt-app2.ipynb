{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "*Course:* [Math 535](http://www.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS)  \n",
    "*Author:* [Sebastien Roch](http://www.math.wisc.edu/~roch/), Department of Mathematics, University of Wisconsin-Madison  \n",
    "***\n",
    "\n",
    "# <span style=\"background-color:dodgerblue; color:white; padding:2px 6px\">APPLICATIONS</span> \n",
    "# Deep neural networks\n",
    "\n",
    "\n",
    "*Updated:* April 16, 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Julia version: 1.3.1\n",
    "ENV[\"JULIA_CUDA_SILENT\"] = true # silences warning about GPUs\n",
    "\n",
    "using CSV, DataFrames, GLM, Statistics, Images, QuartzImageIO\n",
    "using Flux, Flux.Data.MNIST, Flux.Data.FashionMNIST\n",
    "using Flux: mse, train!, Data.DataLoader, throttle\n",
    "using Flux: onehot, onehotbatch, onecold, crossentropy\n",
    "using IterTools: ncycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1 Background\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Today&#39;s paper shows that it is possible to implement John Von Neumann&#39;s claim: &quot;With 4 parameters I can ﬁt an elephant, and with 5 I can make him wiggle his trunk&quot;<br><br>Paper here: <a href=\"https://t.co/SvVrLuRFNy\">https://t.co/SvVrLuRFNy</a> <a href=\"https://t.co/VG37439vE7\">pic.twitter.com/VG37439vE7</a></p>&mdash; Fermat&#39;s Library (@fermatslibrary) <a href=\"https://twitter.com/fermatslibrary/status/965971333422120962?ref_src=twsrc%5Etfw\">February 20, 2018</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this optional notebook, we illustrate the use of automatic differentiation on multiclass classification with deep neural networks. We will not expand on the concepts required here. Review [[Wri](http://www.optimization-online.org/DB_FILE/2016/12/5748.pdf), Section 2.11] first, and then see the following references for background:\n",
    "\n",
    "1. *Convolutional neural networks:* See [[Bis](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Sections 5.1-2, 5.3.1-2, 5.5.6-7] and this [module](http://cs231n.github.io/convolutional-networks/) from Stanford's [CS231n](http://cs231n.github.io/).\n",
    "\n",
    "2. *Automatic differentiation:* See this [Wikipedia article](https://en.wikipedia.org/wiki/Automatic_differentiation#The_chain_rule,_forward_and_reverse_accumulation).\n",
    "\n",
    "3. *Flux.jl:* See the [documentation](https://fluxml.ai/Flux.jl/stable/models/basics/#Model-Building-Basics-1) for the [Flux.jl](https://fluxml.ai) package.\n",
    "\n",
    "We have already used automatic differentiation and Flux.jl in previous notebooks. We introduce more advanced features here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 Linear regression\n",
    "\n",
    "We begin by using [Flux.jl](https://fluxml.ai) on one problem we have encountered previously, linear regression.\n",
    "\n",
    "### 2.1 Flux\n",
    "\n",
    "The `Dense` function constructs an affine map from `in` predictor variables to `out` response variables. It is defined a special way that its parameters, the matrix `W` and the intercept vector `b`, can be accessed by `m.W` and `m.b` if `m` is the name given to the function.  \n",
    "\n",
    "We will also use some other utility functions. The function `mse` computes the mean squared error (MSE). It will be our loss function in this section. The functions `DataLoader` and `ncycle` allow us to construct mini-batches in a straightforward way. Finally `thottle` is used to print progress messages.\n",
    "\n",
    "The help rubric of each of these is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mArray \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVector \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mMatrix \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVecOrMat \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mConvDims\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Dense(in::Integer, out::Integer, σ = identity)\n",
       "\\end{verbatim}\n",
       "Creates a traditional \\texttt{Dense} layer with parameters \\texttt{W} and \\texttt{b}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "y = σ.(W * x .+ b)\n",
       "\\end{verbatim}\n",
       "The input \\texttt{x} must be a vector of length \\texttt{in}, or a batch of vectors represented as an \\texttt{in × N} matrix. The out \\texttt{y} will be a vector or batch of length \\texttt{out}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> d = Dense(5, 2)\n",
       "Dense(5, 2)\n",
       "\n",
       "julia> d(rand(5))\n",
       "Tracked 2-element Array{Float64,1}:\n",
       "  0.00257447\n",
       "  -0.00449443\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "Dense(in::Integer, out::Integer, σ = identity)\n",
       "```\n",
       "\n",
       "Creates a traditional `Dense` layer with parameters `W` and `b`.\n",
       "\n",
       "```\n",
       "y = σ.(W * x .+ b)\n",
       "```\n",
       "\n",
       "The input `x` must be a vector of length `in`, or a batch of vectors represented as an `in × N` matrix. The out `y` will be a vector or batch of length `out`.\n",
       "\n",
       "```julia\n",
       "julia> d = Dense(5, 2)\n",
       "Dense(5, 2)\n",
       "\n",
       "julia> d(rand(5))\n",
       "Tracked 2-element Array{Float64,1}:\n",
       "  0.00257447\n",
       "  -0.00449443\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  Dense(in::Integer, out::Integer, σ = identity)\u001b[39m\n",
       "\n",
       "  Creates a traditional \u001b[36mDense\u001b[39m layer with parameters \u001b[36mW\u001b[39m and \u001b[36mb\u001b[39m.\n",
       "\n",
       "\u001b[36m  y = σ.(W * x .+ b)\u001b[39m\n",
       "\n",
       "  The input \u001b[36mx\u001b[39m must be a vector of length \u001b[36min\u001b[39m, or a batch of vectors represented\n",
       "  as an \u001b[36min × N\u001b[39m matrix. The out \u001b[36my\u001b[39m will be a vector or batch of length \u001b[36mout\u001b[39m.\n",
       "\n",
       "\u001b[36m  julia> d = Dense(5, 2)\u001b[39m\n",
       "\u001b[36m  Dense(5, 2)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> d(rand(5))\u001b[39m\n",
       "\u001b[36m  Tracked 2-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m    0.00257447\u001b[39m\n",
       "\u001b[36m    -0.00449443\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m r\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m i\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ms\u001b[22mtr\u001b[0m\u001b[1me\u001b[22mtch i\u001b[0m\u001b[1mm\u001b[22mre\u001b[0m\u001b[1ms\u001b[22miz\u001b[0m\u001b[1me\u001b[22m Su\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mS\u001b[22mquar\u001b[0m\u001b[1me\u001b[22mdDifference Co\u001b[0m\u001b[1mm\u001b[22mpo\u001b[0m\u001b[1ms\u001b[22mit\u001b[0m\u001b[1me\u001b[22mException\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "mse(ŷ, y)\n",
       "\\end{verbatim}\n",
       "Return the mean squared error \\texttt{sum((ŷ .- y).\\^{}2) / length(y)}. \n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "mse(ŷ, y)\n",
       "```\n",
       "\n",
       "Return the mean squared error `sum((ŷ .- y).^2) / length(y)`. \n"
      ],
      "text/plain": [
       "\u001b[36m  mse(ŷ, y)\u001b[39m\n",
       "\n",
       "  Return the mean squared error \u001b[36msum((ŷ .- y).^2) / length(y)\u001b[39m. "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DataLoader(data...; batchsize=1, shuffle=false, partial=true)\n",
       "\\end{verbatim}\n",
       "An object that iterates over mini-batches of \\texttt{data}, each mini-batch containing \\texttt{batchsize} observations (except possibly the last one). \n",
       "\n",
       "Takes as input one or more data tensors, e.g. X in unsupervised learning, X and Y in  supervised learning. The last dimension in each tensor is considered to be the observation dimension. \n",
       "\n",
       "If \\texttt{shuffle=true}, shuffles the observations each time iterations are re-started. If \\texttt{partial=false}, drops the last mini-batch if it is smaller than the batchsize.\n",
       "\n",
       "The original data is preserved as a tuple in the \\texttt{data} field of the DataLoader. \n",
       "\n",
       "Example usage:\n",
       "\n",
       "\\begin{verbatim}\n",
       "Xtrain = rand(10, 100)\n",
       "train_loader = DataLoader(Xtrain, batchsize=2) \n",
       "# iterate over 50 mini-batches of size 2\n",
       "for x in train_loader: \n",
       "    @assert size(x) == (10, 2)\n",
       "    ...\n",
       "end\n",
       "\n",
       "train_loader.data   # original dataset\n",
       "\n",
       "Xtrain = rand(10, 100)\n",
       "Ytrain = rand(100)\n",
       "train_loader = DataLoader(Xtrain, Ytrain, batchsize=2, shuffle=true) \n",
       "for epoch in 1:100\n",
       "    for (x, y) in train_loader: \n",
       "        @assert size(x) == (10, 2)\n",
       "        @assert size(y) == (2,)\n",
       "        ...\n",
       "    end\n",
       "end\n",
       "\n",
       "# train for 10 epochs\n",
       "using IterTools: ncycle \n",
       "Flux.train!(loss, ps, ncycle(train_loader, 10), opt)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "DataLoader(data...; batchsize=1, shuffle=false, partial=true)\n",
       "```\n",
       "\n",
       "An object that iterates over mini-batches of `data`, each mini-batch containing `batchsize` observations (except possibly the last one). \n",
       "\n",
       "Takes as input one or more data tensors, e.g. X in unsupervised learning, X and Y in  supervised learning. The last dimension in each tensor is considered to be the observation dimension. \n",
       "\n",
       "If `shuffle=true`, shuffles the observations each time iterations are re-started. If `partial=false`, drops the last mini-batch if it is smaller than the batchsize.\n",
       "\n",
       "The original data is preserved as a tuple in the `data` field of the DataLoader. \n",
       "\n",
       "Example usage:\n",
       "\n",
       "```\n",
       "Xtrain = rand(10, 100)\n",
       "train_loader = DataLoader(Xtrain, batchsize=2) \n",
       "# iterate over 50 mini-batches of size 2\n",
       "for x in train_loader: \n",
       "    @assert size(x) == (10, 2)\n",
       "    ...\n",
       "end\n",
       "\n",
       "train_loader.data   # original dataset\n",
       "\n",
       "Xtrain = rand(10, 100)\n",
       "Ytrain = rand(100)\n",
       "train_loader = DataLoader(Xtrain, Ytrain, batchsize=2, shuffle=true) \n",
       "for epoch in 1:100\n",
       "    for (x, y) in train_loader: \n",
       "        @assert size(x) == (10, 2)\n",
       "        @assert size(y) == (2,)\n",
       "        ...\n",
       "    end\n",
       "end\n",
       "\n",
       "# train for 10 epochs\n",
       "using IterTools: ncycle \n",
       "Flux.train!(loss, ps, ncycle(train_loader, 10), opt)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  DataLoader(data...; batchsize=1, shuffle=false, partial=true)\u001b[39m\n",
       "\n",
       "  An object that iterates over mini-batches of \u001b[36mdata\u001b[39m, each mini-batch\n",
       "  containing \u001b[36mbatchsize\u001b[39m observations (except possibly the last one). \n",
       "\n",
       "  Takes as input one or more data tensors, e.g. X in unsupervised learning, X\n",
       "  and Y in supervised learning. The last dimension in each tensor is\n",
       "  considered to be the observation dimension. \n",
       "\n",
       "  If \u001b[36mshuffle=true\u001b[39m, shuffles the observations each time iterations are\n",
       "  re-started. If \u001b[36mpartial=false\u001b[39m, drops the last mini-batch if it is smaller\n",
       "  than the batchsize.\n",
       "\n",
       "  The original data is preserved as a tuple in the \u001b[36mdata\u001b[39m field of the\n",
       "  DataLoader. \n",
       "\n",
       "  Example usage:\n",
       "\n",
       "\u001b[36m  Xtrain = rand(10, 100)\u001b[39m\n",
       "\u001b[36m  train_loader = DataLoader(Xtrain, batchsize=2) \u001b[39m\n",
       "\u001b[36m  # iterate over 50 mini-batches of size 2\u001b[39m\n",
       "\u001b[36m  for x in train_loader: \u001b[39m\n",
       "\u001b[36m      @assert size(x) == (10, 2)\u001b[39m\n",
       "\u001b[36m      ...\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  train_loader.data   # original dataset\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Xtrain = rand(10, 100)\u001b[39m\n",
       "\u001b[36m  Ytrain = rand(100)\u001b[39m\n",
       "\u001b[36m  train_loader = DataLoader(Xtrain, Ytrain, batchsize=2, shuffle=true) \u001b[39m\n",
       "\u001b[36m  for epoch in 1:100\u001b[39m\n",
       "\u001b[36m      for (x, y) in train_loader: \u001b[39m\n",
       "\u001b[36m          @assert size(x) == (10, 2)\u001b[39m\n",
       "\u001b[36m          @assert size(y) == (2,)\u001b[39m\n",
       "\u001b[36m          ...\u001b[39m\n",
       "\u001b[36m      end\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # train for 10 epochs\u001b[39m\n",
       "\u001b[36m  using IterTools: ncycle \u001b[39m\n",
       "\u001b[36m  Flux.train!(loss, ps, ncycle(train_loader, 10), opt)\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "ncycle(iter, n)\n",
       "\\end{verbatim}\n",
       "Cycle through \\texttt{iter} \\texttt{n} times.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> for i in ncycle(1:3, 2)\n",
       "           @show i\n",
       "       end\n",
       "i = 1\n",
       "i = 2\n",
       "i = 3\n",
       "i = 1\n",
       "i = 2\n",
       "i = 3\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "ncycle(iter, n)\n",
       "```\n",
       "\n",
       "Cycle through `iter` `n` times.\n",
       "\n",
       "```jldoctest\n",
       "julia> for i in ncycle(1:3, 2)\n",
       "           @show i\n",
       "       end\n",
       "i = 1\n",
       "i = 2\n",
       "i = 3\n",
       "i = 1\n",
       "i = 2\n",
       "i = 3\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  ncycle(iter, n)\u001b[39m\n",
       "\n",
       "  Cycle through \u001b[36miter\u001b[39m \u001b[36mn\u001b[39m times.\n",
       "\n",
       "\u001b[36m  julia> for i in ncycle(1:3, 2)\u001b[39m\n",
       "\u001b[36m             @show i\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  i = 1\u001b[39m\n",
       "\u001b[36m  i = 2\u001b[39m\n",
       "\u001b[36m  i = 3\u001b[39m\n",
       "\u001b[36m  i = 1\u001b[39m\n",
       "\u001b[36m  i = 2\u001b[39m\n",
       "\u001b[36m  i = 3\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ncycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Returns a function that when invoked, will only be triggered at most once during \\texttt{timeout} seconds. Normally, the throttled function will run as much as it can, without ever going more than once per \\texttt{wait} duration; but if you'd like to disable the execution on the leading edge, pass \\texttt{leading=false}. To enable execution on the trailing edge, ditto.\n",
       "\n"
      ],
      "text/markdown": [
       "Returns a function that when invoked, will only be triggered at most once during `timeout` seconds. Normally, the throttled function will run as much as it can, without ever going more than once per `wait` duration; but if you'd like to disable the execution on the leading edge, pass `leading=false`. To enable execution on the trailing edge, ditto.\n"
      ],
      "text/plain": [
       "  Returns a function that when invoked, will only be triggered at most once\n",
       "  during \u001b[36mtimeout\u001b[39m seconds. Normally, the throttled function will run as much as\n",
       "  it can, without ever going more than once per \u001b[36mwait\u001b[39m duration; but if you'd\n",
       "  like to disable the execution on the leading edge, pass \u001b[36mleading=false\u001b[39m. To\n",
       "  enable execution on the trailing edge, ditto."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?throttle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2.2 `Advertising` dataset and least-squares solution\n",
    "\n",
    "We return to the `Advertising` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>TV</th><th>radio</th><th>newspaper</th><th>sales</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 5 columns</p><tr><th>1</th><td>1</td><td>230.1</td><td>37.8</td><td>69.2</td><td>22.1</td></tr><tr><th>2</th><td>2</td><td>44.5</td><td>39.3</td><td>45.1</td><td>10.4</td></tr><tr><th>3</th><td>3</td><td>17.2</td><td>45.9</td><td>69.3</td><td>9.3</td></tr><tr><th>4</th><td>4</td><td>151.5</td><td>41.3</td><td>58.5</td><td>18.5</td></tr><tr><th>5</th><td>5</td><td>180.8</td><td>10.8</td><td>58.4</td><td>12.9</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Column1 & TV & radio & newspaper & sales\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 230.1 & 37.8 & 69.2 & 22.1 \\\\\n",
       "\t2 & 2 & 44.5 & 39.3 & 45.1 & 10.4 \\\\\n",
       "\t3 & 3 & 17.2 & 45.9 & 69.3 & 9.3 \\\\\n",
       "\t4 & 4 & 151.5 & 41.3 & 58.5 & 18.5 \\\\\n",
       "\t5 & 5 & 180.8 & 10.8 & 58.4 & 12.9 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×5 DataFrame\n",
       "│ Row │ Column1 │ TV      │ radio   │ newspaper │ sales   │\n",
       "│     │ \u001b[90mInt64\u001b[39m   │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼───────────┼─────────┤\n",
       "│ 1   │ 1       │ 230.1   │ 37.8    │ 69.2      │ 22.1    │\n",
       "│ 2   │ 2       │ 44.5    │ 39.3    │ 45.1      │ 10.4    │\n",
       "│ 3   │ 3       │ 17.2    │ 45.9    │ 69.3      │ 9.3     │\n",
       "│ 4   │ 4       │ 151.5   │ 41.3    │ 58.5      │ 18.5    │\n",
       "│ 5   │ 5       │ 180.8   │ 10.8    │ 58.4      │ 12.9    │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(CSV.File(\"./advertising.csv\"))\n",
    "first(df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = nrow(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first compute the solution using the least-squares approach we detailed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = reduce(hcat, [df[:,:TV], df[:,:radio], df[:,:newspaper]])\n",
    "Xaug = hcat(ones(n), X)\n",
    "y = df[:,:sales];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.206607 seconds (3.21 M allocations: 154.619 MiB, 5.52% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  2.938889369459415    \n",
       "  0.04576464545539759  \n",
       "  0.18853001691820445  \n",
       " -0.0010374930424763011"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time q = Xaug\\y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The MSE is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7841263145109356"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean((Xaug*q .- y).^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2.3 Solving the problem using Flux\n",
    "\n",
    "We use `DataLoader` to set up the data for Flux. Note that it takes the transpose of what we have been using, that is, the columns of the data matrix correspond to the samples. Here we take min-batches of size `batchsize=20` and the option `shuffle=true` indicates that we apply a random permutation of the samples on every pass through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = X'\n",
    "ytrain = reshape(y, (1,length(y)))\n",
    "loader = DataLoader(Xtrain, ytrain; batchsize=20, shuffle=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, the first component of the first item is the features for the first 20 samples (after random permutation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×20 Array{Float64,2}:\n",
       " 50.0  188.4  198.9  70.6  25.6  75.5  …  39.5  76.4  280.7  38.2  48.3\n",
       " 11.6   18.1   49.4  16.0  39.0  10.8     41.1   0.8   13.9   3.7  47.0\n",
       " 18.4   25.6   60.0  40.8   9.3   6.0      5.8  14.8   37.0  13.8   8.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(loader)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we construct our model. It is simply an affine map from $\\mathbb{R}^3$ to $\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(3, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Dense(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The loss function is the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y) = mse(m(x),y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, the function [`train!`](https://fluxml.ai/Flux.jl/stable/training/training/#Flux.Optimise.train!) runs an optimization method of our choice on the loss function. The `!` in the function name indicates that it modifies the parameters we pass to it, in this case `m.W` and `m.b`. There are many [optimizers](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Optimiser-Reference-1) available . Stochastic gradient descent can chosen with [Descent](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Descent) but it is slow. Insteadm we will use the popular [ADAM](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM). See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc) for a brief explanation of many common optimizers.\n",
    "\n",
    "We also pass the parameters to `train!` using `params` and a callback function `evalcb()` that prints progress. \n",
    "\n",
    "Choosing the right number of passes (i.e. epochs) through the data requires some experimenting. Here $10^4$ suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ps = params(m)\n",
    "opt = ADAM()\n",
    "evalcb =() -> @show(loss(Xtrain,ytrain));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(Xtrain, ytrain) = 8232.279045854151\n",
      "loss(Xtrain, ytrain) = 2.8080499476318868\n",
      "loss(Xtrain, ytrain) = 2.8222149375067676\n",
      "loss(Xtrain, ytrain) = 2.784338307397086\n",
      "loss(Xtrain, ytrain) = 2.7841753653448005\n",
      "loss(Xtrain, ytrain) = 2.7904427669621223\n",
      "loss(Xtrain, ytrain) = 2.7846128331698248\n",
      " 21.511215 seconds (90.20 M allocations: 3.040 GiB, 4.00% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, Int(1e4)), opt, cb = throttle(evalcb, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final parameters and loss are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float32,1}:\n",
       " 2.9353812"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float32,2}:\n",
       " 0.0455294  0.189447  -0.000817337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7848454944144128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 3 MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we move on to a multicass classification problem. We will use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database):\n",
    "\n",
    "> The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a sample of the images:\n",
    "\n",
    "![MNIST sample images](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first load the data and convert it to an appropriate matrix representation. The data can be accessed with `Flux.Data.MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = MNIST.images()\n",
    "labels = MNIST.labels()\n",
    "length(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, the first image and its label are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                                  \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first transform the images into vectors using `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1me\u001b[22m p\u001b[0m\u001b[1mr\u001b[22momot\u001b[0m\u001b[1me\u001b[22m_\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "reshape(A, dims...) -> AbstractArray\n",
       "reshape(A, dims) -> AbstractArray\n",
       "\\end{verbatim}\n",
       "Return an array with the same data as \\texttt{A}, but with different dimension sizes or number of dimensions. The two arrays share the same underlying data, so that the result is mutable if and only if \\texttt{A} is mutable, and setting elements of one alters the values of the other.\n",
       "\n",
       "The new dimensions may be specified either as a list of arguments or as a shape tuple. At most one dimension may be specified with a \\texttt{:}, in which case its length is computed such that its product with all the specified dimensions is equal to the length of the original array \\texttt{A}. The total number of elements must not change.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = Vector(1:16)\n",
       "16-element Array{Int64,1}:\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  5\n",
       "  6\n",
       "  7\n",
       "  8\n",
       "  9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       "\n",
       "julia> reshape(A, (4, 4))\n",
       "4×4 Array{Int64,2}:\n",
       " 1  5   9  13\n",
       " 2  6  10  14\n",
       " 3  7  11  15\n",
       " 4  8  12  16\n",
       "\n",
       "julia> reshape(A, 2, :)\n",
       "2×8 Array{Int64,2}:\n",
       " 1  3  5  7   9  11  13  15\n",
       " 2  4  6  8  10  12  14  16\n",
       "\n",
       "julia> reshape(1:6, 2, 3)\n",
       "2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\n",
       " 1  3  5\n",
       " 2  4  6\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "reshape(A, dims...) -> AbstractArray\n",
       "reshape(A, dims) -> AbstractArray\n",
       "```\n",
       "\n",
       "Return an array with the same data as `A`, but with different dimension sizes or number of dimensions. The two arrays share the same underlying data, so that the result is mutable if and only if `A` is mutable, and setting elements of one alters the values of the other.\n",
       "\n",
       "The new dimensions may be specified either as a list of arguments or as a shape tuple. At most one dimension may be specified with a `:`, in which case its length is computed such that its product with all the specified dimensions is equal to the length of the original array `A`. The total number of elements must not change.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = Vector(1:16)\n",
       "16-element Array{Int64,1}:\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  5\n",
       "  6\n",
       "  7\n",
       "  8\n",
       "  9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       "\n",
       "julia> reshape(A, (4, 4))\n",
       "4×4 Array{Int64,2}:\n",
       " 1  5   9  13\n",
       " 2  6  10  14\n",
       " 3  7  11  15\n",
       " 4  8  12  16\n",
       "\n",
       "julia> reshape(A, 2, :)\n",
       "2×8 Array{Int64,2}:\n",
       " 1  3  5  7   9  11  13  15\n",
       " 2  4  6  8  10  12  14  16\n",
       "\n",
       "julia> reshape(1:6, 2, 3)\n",
       "2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\n",
       " 1  3  5\n",
       " 2  4  6\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  reshape(A, dims...) -> AbstractArray\u001b[39m\n",
       "\u001b[36m  reshape(A, dims) -> AbstractArray\u001b[39m\n",
       "\n",
       "  Return an array with the same data as \u001b[36mA\u001b[39m, but with different dimension sizes\n",
       "  or number of dimensions. The two arrays share the same underlying data, so\n",
       "  that the result is mutable if and only if \u001b[36mA\u001b[39m is mutable, and setting elements\n",
       "  of one alters the values of the other.\n",
       "\n",
       "  The new dimensions may be specified either as a list of arguments or as a\n",
       "  shape tuple. At most one dimension may be specified with a \u001b[36m:\u001b[39m, in which case\n",
       "  its length is computed such that its product with all the specified\n",
       "  dimensions is equal to the length of the original array \u001b[36mA\u001b[39m. The total number\n",
       "  of elements must not change.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = Vector(1:16)\u001b[39m\n",
       "\u001b[36m  16-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m    1\u001b[39m\n",
       "\u001b[36m    2\u001b[39m\n",
       "\u001b[36m    3\u001b[39m\n",
       "\u001b[36m    4\u001b[39m\n",
       "\u001b[36m    5\u001b[39m\n",
       "\u001b[36m    6\u001b[39m\n",
       "\u001b[36m    7\u001b[39m\n",
       "\u001b[36m    8\u001b[39m\n",
       "\u001b[36m    9\u001b[39m\n",
       "\u001b[36m   10\u001b[39m\n",
       "\u001b[36m   11\u001b[39m\n",
       "\u001b[36m   12\u001b[39m\n",
       "\u001b[36m   13\u001b[39m\n",
       "\u001b[36m   14\u001b[39m\n",
       "\u001b[36m   15\u001b[39m\n",
       "\u001b[36m   16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> reshape(A, (4, 4))\u001b[39m\n",
       "\u001b[36m  4×4 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  5   9  13\u001b[39m\n",
       "\u001b[36m   2  6  10  14\u001b[39m\n",
       "\u001b[36m   3  7  11  15\u001b[39m\n",
       "\u001b[36m   4  8  12  16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> reshape(A, 2, :)\u001b[39m\n",
       "\u001b[36m  2×8 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  3  5  7   9  11  13  15\u001b[39m\n",
       "\u001b[36m   2  4  6  8  10  12  14  16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> reshape(1:6, 2, 3)\u001b[39m\n",
       "\u001b[36m  2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   1  3  5\u001b[39m\n",
       "\u001b[36m   2  4  6\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784-element Array{Float32,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮  \n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(Float32.(imgs[1]),:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Using a list comprehension and `reduce` with `hcat`, we do this for every image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = reduce(hcat, [reshape(Float32.(imgs[i]),:) for i = 1:length(imgs)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can get back the original images by using `reshape` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×28 Array{Gray{Float32},2} with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(reshape(Xtrain[:,1],(28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also convert the labels into vectors. We use [one-hot encoding](https://fluxml.ai/Flux.jl/stable/data/onehot/), that is, we convert the label `0` to the standard basis $\\mathbf{e}_1 \\in \\mathbb{R}^{10}$, the label `1` to $\\mathbf{e}_2 \\in \\mathbb{R}^{10}$, and so on. The functions `onehot` and `onehotbatch` perform this transformation, while `onecold` undoes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "onehotbatch(ls, labels[, unk...])\n",
       "\\end{verbatim}\n",
       "Create an \\href{@ref}{\\texttt{OneHotMatrix}} with a batch of labels based on possible \\texttt{labels} set, returns the \\texttt{onehot(unk, labels)} if given labels \\texttt{ls} is not found in set \\texttt{labels}.\n",
       "\n",
       "\\subsection{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using Flux: onehotbatch\n",
       "\n",
       "julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n",
       "3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  1  0\n",
       " 1  0  1\n",
       " 0  0  0\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "onehotbatch(ls, labels[, unk...])\n",
       "```\n",
       "\n",
       "Create an [`OneHotMatrix`](@ref) with a batch of labels based on possible `labels` set, returns the `onehot(unk, labels)` if given labels `ls` is not found in set `labels`.\n",
       "\n",
       "## Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using Flux: onehotbatch\n",
       "\n",
       "julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n",
       "3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  1  0\n",
       " 1  0  1\n",
       " 0  0  0\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  onehotbatch(ls, labels[, unk...])\u001b[39m\n",
       "\n",
       "  Create an \u001b[36mOneHotMatrix\u001b[39m with a batch of labels based on possible \u001b[36mlabels\u001b[39m set,\n",
       "  returns the \u001b[36monehot(unk, labels)\u001b[39m if given labels \u001b[36mls\u001b[39m is not found in set\n",
       "  \u001b[36mlabels\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ==========\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using Flux: onehotbatch\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\u001b[39m\n",
       "\u001b[36m   0  1  0\u001b[39m\n",
       "\u001b[36m   1  0  1\u001b[39m\n",
       "\u001b[36m   0  0  0\u001b[39m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?onehotbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: SkipC\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22mn\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22mti\u001b[0m\u001b[1mo\u001b[22mn Exp\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22mntialBa\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mO\u001b[22mff c\u001b[0m\u001b[1mo\u001b[22mmpo\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22mnt_\u001b[0m\u001b[1mc\u001b[22mentr\u001b[0m\u001b[1mo\u001b[22mids\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "onecold(y[, labels = 1:length(y)])\n",
       "\\end{verbatim}\n",
       "Inverse operations of \\href{@ref}{\\texttt{onehot}}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using Flux: onecold\n",
       "\n",
       "julia> onecold([true, false, false], [:a, :b, :c])\n",
       ":a\n",
       "\n",
       "julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n",
       ":c\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "onecold(y[, labels = 1:length(y)])\n",
       "```\n",
       "\n",
       "Inverse operations of [`onehot`](@ref).\n",
       "\n",
       "```jldoctest\n",
       "julia> using Flux: onecold\n",
       "\n",
       "julia> onecold([true, false, false], [:a, :b, :c])\n",
       ":a\n",
       "\n",
       "julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n",
       ":c\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  onecold(y[, labels = 1:length(y)])\u001b[39m\n",
       "\n",
       "  Inverse operations of \u001b[36monehot\u001b[39m.\n",
       "\n",
       "\u001b[36m  julia> using Flux: onecold\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onecold([true, false, false], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  :a\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  :c\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?onecold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, on the first label we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Flux.OneHotVector:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(labels[1], 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(ans, 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We do this for all labels simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ytrain = onehotbatch(labels, 0:9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will also use a test dataset provided in MNIST to assess the accuracy of our classifiers. We perform the same transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs = MNIST.images(:test)\n",
    "test_labels = MNIST.labels(:test)\n",
    "length(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Xtest = reduce(hcat, \n",
    "    [reshape(Float32.(test_imgs[i]),:) for i = 1:length(test_imgs)])\n",
    "ytest = onehotbatch(test_labels, 0:9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 4 Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We first appeal to [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to learn a classifier for the MNIST data. See e.g. [[Bis](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), Section 4.3.4] for background. For our purposes, it suffices to say that the model we are using takes the form of an affine map from $\\mathbb{R}^{784}$ to $\\mathbb{R}^{10}$ (where $784 = 28^2$ is the size of the images in vector form and $10$ is the dimension of the one-hot encoding of the labels) composed with the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function which returns a probability distribution over the $10$ labels. The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In Flux, composition of functions can be achieved with `Chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m bat\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mh\u001b[22med_\u001b[0m\u001b[1ma\u001b[22mdjo\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22mt \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1ma\u001b[22mnnelv\u001b[0m\u001b[1mi\u001b[22mew\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Chain(layers...)\n",
       "\\end{verbatim}\n",
       "Chain multiple layers / functions together, so that they are called in sequence on a given input.\n",
       "\n",
       "\\begin{verbatim}\n",
       "m = Chain(x -> x^2, x -> x+1)\n",
       "m(5) == 26\n",
       "\n",
       "m = Chain(Dense(10, 5), Dense(5, 2))\n",
       "x = rand(10)\n",
       "m(x) == m[2](m[1](x))\n",
       "\\end{verbatim}\n",
       "\\texttt{Chain} also supports indexing and slicing, e.g. \\texttt{m[2]} or \\texttt{m[1:end-1]}. \\texttt{m[1:3](x)} will calculate the output of the first three layers.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Chain(layers...)\n",
       "```\n",
       "\n",
       "Chain multiple layers / functions together, so that they are called in sequence on a given input.\n",
       "\n",
       "```julia\n",
       "m = Chain(x -> x^2, x -> x+1)\n",
       "m(5) == 26\n",
       "\n",
       "m = Chain(Dense(10, 5), Dense(5, 2))\n",
       "x = rand(10)\n",
       "m(x) == m[2](m[1](x))\n",
       "```\n",
       "\n",
       "`Chain` also supports indexing and slicing, e.g. `m[2]` or `m[1:end-1]`. `m[1:3](x)` will calculate the output of the first three layers.\n"
      ],
      "text/plain": [
       "\u001b[36m  Chain(layers...)\u001b[39m\n",
       "\n",
       "  Chain multiple layers / functions together, so that they are called in\n",
       "  sequence on a given input.\n",
       "\n",
       "\u001b[36m  m = Chain(x -> x^2, x -> x+1)\u001b[39m\n",
       "\u001b[36m  m(5) == 26\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  m = Chain(Dense(10, 5), Dense(5, 2))\u001b[39m\n",
       "\u001b[36m  x = rand(10)\u001b[39m\n",
       "\u001b[36m  m(x) == m[2](m[1](x))\u001b[39m\n",
       "\n",
       "  \u001b[36mChain\u001b[39m also supports indexing and slicing, e.g. \u001b[36mm[2]\u001b[39m or \u001b[36mm[1:end-1]\u001b[39m. \u001b[36mm[1:3](x)\u001b[39m\n",
       "  will calculate the output of the first three layers."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Hence our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(784, 10), softmax)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    Dense(28^2, 10), \n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At initialization, the parameters are set randomly.\n",
    "\n",
    "For example, one the first sample, we get the following probability distribution over labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float32,1}:\n",
       " 0.16761522 \n",
       " 0.07530861 \n",
       " 0.048036132\n",
       " 0.06496426 \n",
       " 0.06505113 \n",
       " 0.067898236\n",
       " 0.064455174\n",
       " 0.12192501 \n",
       " 0.15675691 \n",
       " 0.16798933 "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(Xtrain[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We also define a function which computes the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "accuracy(x, y) = mean(onecold(m(x), 0:9) .== onecold(y, 0:9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With random initialization, the current accuracy on the test dataset is close to $10\\%$, as one would expect from a purely random guess among $10$ choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1071"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We are now ready to make mini-batches and set the parameters of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(Xtrain, ytrain; batchsize=128, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss(x, y) = crossentropy(m(x), y)\n",
    "ps = params(m)\n",
    "opt = ADAM()\n",
    "evalcb = () -> @show(accuracy(Xtest,ytest));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We run ADAM for $10$ epochs. You can check for yourself that running it much longer does not lead to much improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(Xtest, ytest) = 0.1236\n",
      "accuracy(Xtest, ytest) = 0.9168\n",
      "accuracy(Xtest, ytest) = 0.9225\n",
      "  9.582298 seconds (29.52 M allocations: 5.115 GiB, 8.11% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, Int(1e1)), opt, cb = throttle(evalcb, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To make a prediction, we use `m(x)` which returns a probability distribution over the $10$ labels. The function `onecold` then returns the label with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float32,1}:\n",
       " 9.711117e-6  \n",
       " 2.5835983e-10\n",
       " 1.8661505e-5 \n",
       " 0.00591915   \n",
       " 8.1335e-7    \n",
       " 3.3519187e-5 \n",
       " 1.5954298e-9 \n",
       " 0.99363446   \n",
       " 3.087511e-5  \n",
       " 0.000352789  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(Xtest[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(ans, 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The true label in that case was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(ytest[:,1], 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                                  \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(test_imgs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 4 Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now consider a more complex model by adding layers, that is, by composing with more affine functions. In between the affine maps, we introduce a nonlinear activation function, in this case [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(784, 32, relu), Dense(32, 10), softmax)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "  Dense(28^2, 32, relu),\n",
    "  Dense(32, 10),\n",
    "  softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We then proceed as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(Xtrain, ytrain; batchsize=128, shuffle=true)\n",
    "accuracy(x, y) = mean(onecold(m(x), 0:9) .== onecold(y, 0:9))\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "ps = params(m)\n",
    "opt = ADAM()\n",
    "evalcb = () -> @show(accuracy(Xtest,ytest));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(Xtest, ytest) = 0.1441\n",
      "accuracy(Xtest, ytest) = 0.9458\n",
      "accuracy(Xtest, ytest) = 0.9569\n",
      "accuracy(Xtest, ytest) = 0.9621\n",
      "  8.793740 seconds (23.02 M allocations: 6.079 GiB, 7.22% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, Int(1e1)), opt, cb = throttle(evalcb, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final accuracy is better than that achieved with multiclass logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9654"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As before, to make a prediction, we use `m(x)` and `onecold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(m(Xtest[:,1]), 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(ytest[:,1], 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 5 Convolutional neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we consider a class of neural networks tailored for image processing, [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN). From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):\n",
    "\n",
    "> In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\n",
    "\n",
    "More background can be found in this excellent [module](http://cs231n.github.io/convolutional-networks/) from Stanford's [CS231n](http://cs231n.github.io/).\n",
    "\n",
    "We will use CNNs on the MNIST dataset. What follows is based on Flux's [model zoo](https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl). Our CNN will be a composition of [convolutional layers](http://cs231n.github.io/convolutional-networks/#conv) and [pooling layers](http://cs231n.github.io/convolutional-networks/#pool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m! \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mert \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mDims \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mexhull \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mTranspose ∇\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m_data\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Conv(size, in=>out)\n",
       "Conv(size, in=>out, relu)\n",
       "\\end{verbatim}\n",
       "Standard convolutional layer. \\texttt{size} should be a tuple like \\texttt{(2, 2)}. \\texttt{in} and \\texttt{out} specify the number of input and output channels respectively.\n",
       "\n",
       "Example: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\\begin{verbatim}\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv((2, 2), 1=>16, relu)\n",
       "\\end{verbatim}\n",
       "Data should be stored in WHCN order (width, height, \\# channels, batch size). In other words, a 100×100 RGB image would be a \\texttt{100×100×3×1} array, and a batch of 50 would be a \\texttt{100×100×3×50} array.\n",
       "\n",
       "Takes the keyword arguments \\texttt{pad}, \\texttt{stride} and \\texttt{dilation}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Conv(size, in=>out)\n",
       "Conv(size, in=>out, relu)\n",
       "```\n",
       "\n",
       "Standard convolutional layer. `size` should be a tuple like `(2, 2)`. `in` and `out` specify the number of input and output channels respectively.\n",
       "\n",
       "Example: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "```\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv((2, 2), 1=>16, relu)\n",
       "```\n",
       "\n",
       "Data should be stored in WHCN order (width, height, # channels, batch size). In other words, a 100×100 RGB image would be a `100×100×3×1` array, and a batch of 50 would be a `100×100×3×50` array.\n",
       "\n",
       "Takes the keyword arguments `pad`, `stride` and `dilation`.\n"
      ],
      "text/plain": [
       "\u001b[36m  Conv(size, in=>out)\u001b[39m\n",
       "\u001b[36m  Conv(size, in=>out, relu)\u001b[39m\n",
       "\n",
       "  Standard convolutional layer. \u001b[36msize\u001b[39m should be a tuple like \u001b[36m(2, 2)\u001b[39m. \u001b[36min\u001b[39m and \u001b[36mout\u001b[39m\n",
       "  specify the number of input and output channels respectively.\n",
       "\n",
       "  Example: Applying Conv layer to a 1-channel input using a 2x2 window size,\n",
       "  giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\u001b[36m  size = (2,2)\u001b[39m\n",
       "\u001b[36m  in = 1\u001b[39m\n",
       "\u001b[36m  out = 16\u001b[39m\n",
       "\u001b[36m  Conv((2, 2), 1=>16, relu)\u001b[39m\n",
       "\n",
       "  Data should be stored in WHCN order (width, height, # channels, batch size).\n",
       "  In other words, a 100×100 RGB image would be a \u001b[36m100×100×3×1\u001b[39m array, and a\n",
       "  batch of 50 would be a \u001b[36m100×100×3×50\u001b[39m array.\n",
       "\n",
       "  Takes the keyword arguments \u001b[36mpad\u001b[39m, \u001b[36mstride\u001b[39m and \u001b[36mdilation\u001b[39m."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m! ∇\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m ∇\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "MaxPool(k)\n",
       "\\end{verbatim}\n",
       "Max pooling layer. \\texttt{k} stands for the size of the window for each dimension of the input.\n",
       "\n",
       "Takes the keyword arguments \\texttt{pad} and \\texttt{stride}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "MaxPool(k)\n",
       "```\n",
       "\n",
       "Max pooling layer. `k` stands for the size of the window for each dimension of the input.\n",
       "\n",
       "Takes the keyword arguments `pad` and `stride`.\n"
      ],
      "text/plain": [
       "\u001b[36m  MaxPool(k)\u001b[39m\n",
       "\n",
       "  Max pooling layer. \u001b[36mk\u001b[39m stands for the size of the window for each dimension of\n",
       "  the input.\n",
       "\n",
       "  Takes the keyword arguments \u001b[36mpad\u001b[39m and \u001b[36mstride\u001b[39m."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)\n",
    "    # which is where we get the 288 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "\n",
    "    # Finally, softmax to get nice probabilities\n",
    "    softmax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " One complication is that the convolutional layers take as input a [tensor](https://en.wikipedia.org/wiki/Tensor), that is, a multidimensional array. So the first step is to convert the images in the dataset into $4d$-arrays in WHCN order (width, height, #channels, batch size). Here the number of of channels is $1$ for grayscale and the batch size is $1$ for a single image. We will use `DataLoader` as before to create larger mini-batches.\n",
    " \n",
    " We use `reshape` to make a $4d$-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×1×1 Array{Float32,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.498039  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.25098   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " ⋮                             ⋮         ⋱                 ⋮            \n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.215686  0.67451      0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.533333  0.992157     0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(Float32.(imgs[1]), 28, 28, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then applying our model outputs a probability distribution over $10$ labels as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1 Array{Float32,2}:\n",
       " 0.10375852 \n",
       " 0.11586949 \n",
       " 0.10154771 \n",
       " 0.09286154 \n",
       " 0.09851568 \n",
       " 0.093938105\n",
       " 0.096605465\n",
       " 0.09569039 \n",
       " 0.096764326\n",
       " 0.10444884 "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We concatenate the images into a large $4d$ tensor where the last dimension is for the samples. Here we cannot use `hcat`, as we are concatenating tensors rather than vectors. Instead we pre-allocate the tensor and then assign the images as we scan the last dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_tensor_imgs = zeros(Float32, 28, 28, 1, length(labels))\n",
    "for i in 1:length(labels)\n",
    "    train_tensor_imgs[:, :, :, i] = reshape(Float32.(imgs[i]), 28, 28, 1, 1)\n",
    "end\n",
    "train_onehot_labels = ytrain;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, the first image is encoded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×1×1 Array{Float32,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.498039  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.25098   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " ⋮                             ⋮         ⋱                 ⋮            \n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.215686  0.67451      0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.533333  0.992157     0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor_imgs[:,:,:,1:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We do the same transformation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_tensor_imgs = zeros(Float32, 28, 28, 1, length(test_labels))\n",
    "for i in 1:length(test_labels)\n",
    "    test_tensor_imgs[:, :, :, i] = reshape(Float32.(test_imgs[i]), 28, 28, 1, 1)\n",
    "end\n",
    "test_onehot_labels = ytest;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now use `DataLoader` to create mini-batches and set the parameters for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(train_tensor_imgs, train_onehot_labels; \n",
    "    batchsize=128, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy(x, y) = mean(onecold(m(x), 0:9) .== onecold(y, 0:9))\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "ps = params(m)\n",
    "opt = ADAM()\n",
    "evalcb = () -> @show(accuracy(test_tensor_imgs, test_onehot_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As before the initial accuracy of the network, with random weights, is close to $10\\%$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0829"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_tensor_imgs,test_onehot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We train for $10$ epochs. On my computer it takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.0915\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9671\n",
      " 75.183072 seconds (17.04 M allocations: 29.925 GiB, 5.30% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, 1), opt, cb = throttle(evalcb, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9677\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9829\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9831\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9885\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9871\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9905\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9885\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9897\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9907\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.9896\n",
      "592.169958 seconds (42.48 M allocations: 250.289 GiB, 4.87% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, 9), opt, cb = throttle(evalcb, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final accuracy is significantly higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.714342 seconds (218.84 k allocations: 1.715 GiB, 1.80% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9903"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time accuracy(test_tensor_imgs, test_onehot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Int64,1}:\n",
       " 7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor_img = reshape(Float32.(test_imgs[1]), 28, 28, 1, 1)\n",
    "onecold(m(new_tensor_img), 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(ytest[:,1], 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 6 Fashion MNIST dataset\n",
    "\n",
    "Finally, we test CNNs on the Fashion MNIST dataset. Quoting [Kaggle](https://www.kaggle.com/zalando-research/fashionmnist):\n",
    "\n",
    "> Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. [...] Each training and test example is assigned to one of the following labels: 0. T-shirt/top, 1. Trouser, 2. Pullover, 3. Dress, 4. Coat, 5. Sandal, 6. Shirt, 7. Sneaker, 8. Bag, 9. Ankle boot.\n",
    "\n",
    "Here are sample images:\n",
    "\n",
    "![Fashion MNIST sample](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)\n",
    "\n",
    "([Source](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The data can be accessed from `Flux.Data.FashionMNIST`. As we did before, we load the training and test data, convert it to tensor form and create mini-batches. We also use the same CNN network structure and run ADAM on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = FashionMNIST.images()\n",
    "labels = FashionMNIST.labels()\n",
    "length(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_tensor_imgs = zeros(Float32, 28, 28, 1, length(labels))\n",
    "for i in 1:length(labels)\n",
    "    train_tensor_imgs[:, :, :, i] = reshape(Float32.(imgs[i]), 28, 28, 1, 1)\n",
    "end\n",
    "train_onehot_labels = onehotbatch(labels, 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs = FashionMNIST.images(:test)\n",
    "test_labels = FashionMNIST.labels(:test)\n",
    "length(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_tensor_imgs = zeros(Float32, 28, 28, 1, length(test_labels))\n",
    "for i in 1:length(test_labels)\n",
    "    test_tensor_imgs[:, :, :, i] = reshape(Float32.(test_imgs[i]), 28, 28, 1, 1)\n",
    "end\n",
    "test_onehot_labels = onehotbatch(test_labels, 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)\n",
    "    # which is where we get the 288 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "\n",
    "    # Finally, softmax to get nice probabilities\n",
    "    softmax,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "accuracy(x, y) = mean(onecold(m(x), 0:9) .== onecold(y, 0:9))\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "ps = params(m)\n",
    "opt = ADAM()\n",
    "evalcb = () -> @show(accuracy(test_tensor_imgs, test_onehot_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0475"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_tensor_imgs,test_onehot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(train_tensor_imgs, train_onehot_labels; \n",
    "    batchsize=128, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.0802\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8308\n",
      " 67.674163 seconds (5.98 M allocations: 29.386 GiB, 4.86% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, 1), opt, cb = throttle(evalcb, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8329\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8525\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8736\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8804\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8763\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.888\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8927\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8912\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8981\n",
      "accuracy(test_tensor_imgs, test_onehot_labels) = 0.8946\n",
      "606.127817 seconds (42.48 M allocations: 250.289 GiB, 4.87% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time train!(loss, ps, ncycle(loader, 9), opt, cb = throttle(evalcb, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.423023 seconds (218.84 k allocations: 1.715 GiB, 2.59% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8973"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time accuracy(test_tensor_imgs, test_onehot_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
